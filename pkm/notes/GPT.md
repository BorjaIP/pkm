---
title: GPT
created: Tuesday 20th June 2023 18:30
aliases: 
tags:
  - gpt
  - llm
---
**GPT** is a family of AI models built by OpenAI. It stands for Generative Pre-trained Transformer, which is basically a description of what the AI models do and how they work (I'll dig into that more in a minute).

| Tool                                                       | Description                                                                                              |
| ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| [Lit-GPT](https://github.com/Lightning-AI/lit-gpt)         | Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training |
| [NExT-GPT](https://github.com/NExT-GPT/NExT-GPT)           | Code and models for NExT-GPT: Any-to-Any Multimodal Large Language Model                                 |
| [PentestGPT](https://github.com/GreyDGL/PentestGPT)        | A GPT-empowered penetration testing tool                                                                 |
| [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) | AutoGPT is the vision of accessible AI for everyone, to use and to build on                              |
# Articles/Talks

- [[ChatGPT DAN, Jailbreaks prompt]]
- [[Neural Databases - A Next Generation Context Retrieval System for Building Specialized AI-Agents with ChatGPT — Part 2-3]]
- [[Why is GPT-3 15.77x more expensive for certain languages]]
- [[Building a Privacy-Preserving LLM-Based Chatbot]]
- [[I tried… Using ChatGPT for FuNn as a DevOps Engineer]]

<iframe width="560" height="315" src="https://www.youtube.com/embed/6K1lyyzpxtk?si=1DGB5dUijjRROOgI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
# Frameworks

- [[PrivateGPT]]
- [[ColossalAI]]
- [SeamlessM4T](https://github.com/facebookresearch/seamless_communication)
# Fine-tunning

- [Falcon – A guide to finetune and inference](https://lightning.ai/blog/falcon-a-guide-to-finetune-and-inference/)
- [How To Finetune GPT Like Large Language Models on a Custom Dataset](https://lightning.ai/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/)
- [Efficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection for Large Language Models](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms)
- [Mistral-7B Fine-Tuning: A Step-by-Step Guide](https://gathnex.medium.com/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8)
- [Fine-tuning Mistral 7B Model with Your Custom Data](https://python.plainenglish.io/intruct-fine-tuning-mistral-7b-model-with-your-custom-data-7eb22921a483)
- [Optimizing LLMs: A Step-by-Step Guide to Fine-Tuning with PEFT and QLoRA](https://blog.lancedb.com/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b)
- [Argilla - Bringing LLM Fine-Tuning and RLHF to Everyone](https://argilla.io/blog/argilla-for-llms/)

## TRL

- [TRL - Transformer Reinforcement Learning](https://github.com/huggingface/trl#trl---transformer-reinforcement-learning)

## Lora

- [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora)
- [Bitsandbytes](https://github.com/TimDettmers/bitsandbytes) - The bitsandbytes is a lightweight wrapper around CUDA custom functions
- [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

# ChatGPT + Azure OpenAI

- [[ChatGPT + Enterprise data with Azure OpenAI and AI Search]]